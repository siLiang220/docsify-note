
## 一、常见的图像生成算法

### 1.1 变分自编码(Variational Autoencoder, VAE)
**自编码器(Auto Encoder)**：自己训练自己与普通神经网络的区别是由两部分构成Encoder 和Decoder ，输出的是重建后的图像，与输入层中有相同的神经元，隐藏层可以对输入进行过滤和特征提取，用较少的数量表示输入的数据。例：谷歌提取图像Encoder 提取特征在用户的手机上Decoder还原图像

![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202312301350453.png)



**变分自编码器 (Variational Autoencoder, VAE)** 是自编码器的一种扩展，它通过引入潜在变量和变分推断来训练生成模型。相较于自编码器将输入映射到固定的潜在表示，VAE将输入映射到一个概率分布上，例如混合高斯分布(GMM)，获取潜在空间中各个特征的均值和方差来描述潜在空间的特征，生成图片是在潜在空间中对每个特征进行采用。其核心目标是学习数据的潜在分布，使得能够生成具有相似特征的新样本。

![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202312301351713.png)

![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202312301334054.png)

#### 应用场景
去水印，抠图、替换背景、图像分割
### 1.2 生成对抗网络（GAN）

![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202312301414609.png)

### 1.3 扩散模型（Diffusionc Model）
**扩散模型** 简单的讲就是通过神经网络学习从纯噪声数据逐渐对数据进行去噪的过程，从噪声中恢复的是原始图像的变种图像
![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202312301522615.png)
#### 应用场景

图像超分、图像上色、文本生成图片、全景图像生成



## Stable Diffusion 训练
### Dreambooth 训练

Dreambooth  主要是用于文生图模型进行微调，只需要根据自己的风格收集图片，然后使用Dreambooth就可以训练一个专属的模型

![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/20231228191329.png)

### Dreambooth 训练过程

> **期望预训练过程：**
> 
> 在预训练模型中，权重被锁定，不会在Dreambooth的训练过程中发生变化。根据提示词“A dog”，默认生成松狮狗这一类型的图片。

1. **获取预训练模型：**
    - 获取一个预训练模型，其中的权重是被锁定的，不会发生改变。
    - 针对提示词"A dog"，预训练模型会生成不同类型的狗图片。
2. **复制并修改权重：**
    - 复制锁定的预训练模型，生成一个新的模型，其中的权重是可以修改的。
    - 复制的模型与原始锁定的模型在同一提示词下生成的结果是相似的（通过损失函数进行对比）。
3. **专属提示词定义：**
    - 在训练自己风格的模型时，定义一个专属的提示词 [V]。
    - 专属提示词是不具有特定意义的词语，这个词语是自己专有风格的提示词。
4. **训练过程：**
    - 训练自己风格的模型，计算自己收集的训练图片与提示词生成的图片之间的损失。
    - 损失值越小越好，使训练模型能够生成符合自己专有风格的图片。
5. **预测与生成：**
    - 使用训练好的模型，输入关键词，生成相应风格的装饰图片。
### Dreambooth 代码结构
![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/20231228200222.png)

#### Dreambooth 训练准备

![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202401010911370.png)

尽量使用不同的环境、灯光、发型、表情、姿势、角度等，以png的格式保存并将这些数据放入文件夹train_images中。

#### Dreambooth 训练代码
```sh
# 导入必要的模块
import sys
import os

# 将miniconda环境添加到系统路径中
os.environ["PATH"] = f'/root/miniconda3/envs/diffusers/bin:{os.environ["PATH"]}'
# 设置Hugging Face缓存路径
os.environ["HF_HOME"] = ".cache"
# 获取当前工作路径
DB_SCRIPT_WORK_PATH = os.getcwd() 

# 定义训练脚本、转换脚本和反向转换脚本的文件名
TRAINER = "train_dreambooth.py"
# 将stable diffusion 转换为diffusers格式
CONVERTER = "convert_v3.py"
# 将diffusers格式 转为stable diffusion
BACK_CONVERTER = "back_convert.py"

# 定义源模型路径和目标模型路径
SRC_PATH = "./weights-sd"
MODEL_NAME = "./weights-hf"

# 定义模型保存路径
OUTPUT_DIR = "./result"
# 创建输出目录（如果不存在）
!mkdir -p $OUTPUT_DIR"
```

`Stable Diffusion`模型转换为`Diffusers`模型
```sh
# 源模型的检查点路径
SOURCE_CHECKPOINT_PATH = f"./model-sd/model.safetensors"
# 使用转换脚本将 SafetyNet 模型转换为 Hugging Face 模型
# --checkpoint_path 指定源模型的检查点路径
# --dump_path 指定目标 Hugging Face 模型的保存路径
!python $CONVERTER --checkpoint_path $SOURCE_CHECKPOINT_PATH --dump_path $MODEL_NAME
```
#### DreamBooth 提示词配置

`INSTANCE_PROMPT` : 专属提示词
`CLASS_PROMPT`： 让AI自动生成image的提示词，不要包含专属提示词
`SAVE_SAMPLE_PROMPT`:  进行多次迭代生成result 图片的提示词 要包含专属提示词
```sh
# 实例提示文本
INSTANCE_PROMPT = "Plaidshirtprogrammer"
# 实例数据目录路径
INSTANCE_DIR = "./train-images"

# 类别图片设置
# 类别提示文本
CLASS_PROMPT = "masterpiece, best quality, 1man"
# 负类别提示文本
CLASS_NEGATIVE_PROMPT = "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry"
# 类别图片数据目录路径
CLASS_DIR = "./generate-images"

# 预览图标签设置即生成結果图片提示词
# 预览图正类别提示文本
SAVE_SAMPLE_PROMPT = "masterpiece, best quality, Plaidshirtprogrammer, looking at viewer"
# 预览图负类别提示文本
SAVE_SAMPLE_NEGATIVE_PROMPT = "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry"

```

#### DreamBooth加速器配置

```sh
!./accelerate.sh
```
该配置文件包含了一些与 Accelerated Inference 相关的默认配置，如计算环境、分布式训练设置、混合精度等。这样的配置文件通常用于定义加速推理的运行环境和选项：
```sh
# 创建目录，确保 Hugging Face Accelerated Inference API 的配置文件目录存在
mkdir -p ~/.cache/huggingface/accelerate

# 将默认的配置信息写入配置文件 default_config.yaml
cat > ~/.cache/huggingface/accelerate/default_config.yaml <<- EOM
compute_environment: LOCAL_MACHINE  # 计算环境设为本地机器
deepspeed_config: {}  # DeepSpeed 配置为空
distributed_type: 'NO'  # 不使用分布式训练
downcast_bf16: 'no'  # 不使用 bf16 数据类型
fsdp_config: {}  # FSDP 配置为空
machine_rank: 0  # 机器的排名设为 0
main_process_ip: null  # 主进程 IP 设为空
main_process_port: null  # 主进程端口设为空
main_training_function: main  # 主训练函数设为 main
mixed_precision: fp16  # 使用混合精度 fp16
num_machines: 1  # 使用的机器数设为 1
num_processes: 1  # 使用的进程数设为 1
use_cpu: false  # 不使用 CPU
EOM
```
#### DreamBooth常用参数配置（这个需要重新看视频）
```sh
# 最大训练步数,
max_train_steps = 1500,
# 学习率设置,
learning_rate = 5e-6,
# 学习率衰减调整策略,
# 可选值: [linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup, cosine_with_restarts_mod, cosine_mod],
lr_scheduler = "cosine_with_restarts",
# 学习率预热步数,指上升到learning_rate的步数
lr_warmup_steps = 100,
# 训练批量大小,每次训练送入的图片数量
train_batch_size = 1,
# 预训练模型自动生成的 class_images 数量,
num_class_images = 20,
# 是否使用 prior preservation（先验保持）, true预训练模型生成图片，false可以使用自己的图片作为class image训练
with_prior_preservation = True,
# 是否训练文本编码器,
train_text_encoder = False,
# 是否使用 ARB（Aspect Ratio Bucket）送入的训练图片是否宽高相同，如果长宽不一样的图片是以长边为准，false:宽高相同
use_aspect_ratio_bucket = False,
# 从文件名读取 prompt,
read_prompt_from_filename = False,
# 从 txt 文件读取 prompt,
read_prompt_from_txt = False,
# 追加提示文本, 是否在之前设置的提示词追加
append_prompt = "instance",
# 保存模型间隔步数即500个保存一次,
save_interval = 500,
# 是否使用 deepdanbooru,
use_deepdanbooru = False
```

不常设置的参数
```sh
# 输入给模型的图片`高度`，由于用户输入的并不是固定大小的图片，因此代码中会将原始大小的图片压缩成指定`高度`的图片，默认值为`None`。
height=512
# 输入给模型的图片`宽度`，由于用户输入的并不是固定大小的图片，因此代码中会将原始大小的图片压缩成指定`宽度`的图片，默认值为`None`。
width=512
# 输入给模型的训练集图片分辨率，当高度和宽度为none时，将会使用 resolution,
resolution = 512,
# 梯度积累步骤,
gradient_accumulation_steps = 1,
# 随机数种子为了重现训练结果,
seed = 1337,
# 日志记录间隔迭代次数,
log_interval = 10,
# 跳过梯度剪裁,
clip_skip = 1,
# 生成样本的批量大小,
sample_batch_size = 4,
# 先验损失的权重,
prior_loss_weight = 1.0,
# 是否缩放学习率,
scale_lr = False,
# 是否使用学习率平方根缩放,
scale_lr_sqrt = False,
# 是否启用梯度检查点,
gradient_checkpointing = True,
# 是否对 tokens 进行填充,
pad_tokens = False,
# 调试 ARB（Aspect Ratio Bucket）,
debug_arb = False,
# 调试提示文本,
debug_prompt = False,
# 是否使用指数移动平均（EMA）,
use_ema = False,
# 训练周期数,
restart_cycle = 1,
# 上一训练周期的结束步数,
last_epoch = -1
```

####  DreamBooth执行模型训练
```sh
!python -m accelerate.commands.launch $TRAINER \
--mixed_precision="fp16" \
# 预训练模型位置
--pretrained_model_name_or_path=$MODEL_NAME \
# 训练集路径
--instance_data_dir=$INSTANCE_DIR \
# 预训练模型生成的图片位置
--class_data_dir=$CLASS_DIR \
# 输出结果路径
--output_dir=$OUTPUT_DIR \
--instance_prompt="{INSTANCE_PROMPT}" \
--class_prompt="{CLASS_PROMPT}" \
--class_negative_prompt="{CLASS_NEGATIVE_PROMPT}" \
--save_sample_prompt="{SAVE_SAMPLE_PROMPT}" \
--save_sample_negative_prompt="{SAVE_SAMPLE_NEGATIVE_PROMPT}" \
--seed=$seed \
--resolution=$resolution \
--train_batch_size=$train_batch_size \
--gradient_accumulation_steps=$gradient_accumulation_steps \
--learning_rate=$learning_rate \
--lr_scheduler=$lr_scheduler \
--lr_warmup_steps=$lr_warmup_steps \
--num_class_images=$num_class_images \
--sample_batch_size=$sample_batch_size \
--max_train_steps=$max_train_steps \
--save_interval=$save_interval \
--log_interval=$log_interval \
--clip_skip $clip_skip \
--num_cycle=$restart_cycle \
--last_epoch=$last_epoch \
--append_prompt=$append_prompt \
--use_8bit_adam --xformers $da_arg $db_arg $ema_arg \
 $ppl_arg $wandb_arg $extra_prompt_arg $gdc_arg $arb_arg $tte_arg $scale_lr_arg $dp_arg $pd_arg
```

训练输出：每500个迭代保存一次`checkPoint`，每保存一次模型会生成4个预览图
![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202312311257042.png)

#### 模型转换

`Dreambooth`生成的模型权重只能在`diffusion`中使用无法在`stable diffusion` 中使用，需要将其转为ckpt形式，转换代码
```sh
# 定义模型文件夹名称
model_folder_name = "checkpoint_last"
# 定义转换后的模型路径
convert_model_path = "f'result/{model_folder_name}"
# 定义检查点路径
ckpt_path = f'{convert_model_path}/model.ckpt'
# 是否保存为半精度模型（默认为保存单精度模型）
save_half = True
# 根据是否保存半精度模型生成相应的命令行参数
ckpt_convert_half_arg = "--half" if save_half else "" 
# 使用 back_convert.py 脚本将模型转换为 TensorFlow 格式
!python back_convert.py --model_path $convert_model_path --checkpoint_path $ckpt_path $ckpt_convert_half_arg
# 打印转换后的模型保存路径
print(f"[*] 转换的模型保存在如下路径 {ckpt_path}")
```
#### 参考文档 
[我的婚纱我做主！ 使用Lora技术用Dreambooth训练你的专属婚纱 - 飞桨AI Studio星河社区 (baidu.com)](https://aistudio.baidu.com/projectdetail/6504885)

### LoRA训练
`LoRA`训练基于`kohya_ss`实现，显存需要8G以上。可以理解为stable diffusion（SD)模型的一种插件，和`hyper-network`，`controlNet`一样，都是在不修改SD模型的前提下，利用少量数据训练出一种画风/IP/人物

![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202312312202075.png)

#### loRA 代码结构
![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202401010842996.png)

- output: 输出的模型权重
- sd-models: 预训练的模型，对该模型生成的结果进行微调 
- train: 训练集数据包含训练图片和图片的描述信息
#### LoRA数据集准备

对`Dreambooth`训练的结果进行微小的调增

![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202401011004861.png)
#### LoRA训练参数

`train.sh`脚本配置
```sh
# 训练数据路径设置
pretrained_model = "./sd-models/model-plaidshirtprogrammer.ckpt"  # 预训练模型路径（这里是使用dreambooth训练的格子衫模型）
is_v2_model = 0                             # SD2.0模型标志 (0表示SD1.0，1表示SD2.0)
parameterization = 0                        # 参数化标志，实验性功能
train_data_dir = "./train/lora-glass"        # 训练数据集路径
reg_data_dir = ""                            # 正则化图像目录，默认不使用正则化图像

# 网络设置
network_module = "networks.lora"             # 网络模块，LoRA训练为默认值"networks.lora"
network_weights = ""                         # LoRA网络预训练权重路径
network_dim = 32                             # 网络维度，通常为4到128之间
network_alpha = 32                           # 网络alpha参数，通常与network_dim相同或较小以防止下溢，默认为1


# 训练相关参数
resolution = "512,512"       # 送入网络图像分辨率，宽和高。支持非正方形，但必须是 64 的倍数。
batch_size = 1               # 每个训练批次的样本数量。
max_train_epoches = 10       # 每个图片最大训练轮数。
save_every_n_epochs = 2      # 每隔 N 个轮次保存一次模型。

# 训练模式：
train_unet_only = 0          # 是否仅训练 U-Net。开启此选项可能会减少显存使用，但可能会牺牲模型效果。
train_text_encoder_only = 0  # 是否仅训练文本编码器。
stop_text_encoder_training = 0 # 在第 N 步时停止训练文本编码器。

# 噪声和标记处理：
noise_offset = "0"           # 在训练中添加噪声偏移，以改善生成非常暗或非常亮的图像。如果启用，建议参数为 0.1。
keep_tokens = 0              # 在随机打乱标记时，保留前 N 个不变的标记。
min_snr_gamma = 0            # 伽马射线事件的最小信噪比（SNR）值，默认为 0。

# 学习率设置
lr = "1e-4"                     # 整体学习率，用于控制整个模型的学习速率
unet_lr = "1e-4"                 # U-Net 部分的学习率
text_encoder_lr = "1e-5"         # 文本编码器的学习率
lr_scheduler = "cosine_with_restarts"  # 学习率调度器的类型，选择了余弦退火重启模式，可选还包括 "linear", "cosine", "polynomial", "constant", "constant_with_warmup", "adafactor"
lr_warmup_steps = 0              # 学习率预热步数，当学习率调度器为 "constant" 或 "adafactor" 时，该值需要设为 0。
lr_restart_cycles = 1            # 余弦退火的重启周期数，仅在学习率调度器为 "cosine_with_restarts" 时生效。

# 优化器设置
optimizer_type = "AdamW8bit"     # 优化器类型，默认为 8bitadam，其他可选项包括 "AdamW", "AdamW8bit",


# 输出设置
output_name = "glass"            # 输出模型的名称，用于保存训练得到的模型。
save_model_as = "safetensors"  # 模型保存的格式，可选为 "ckpt"、"pt"、"safetensors" 等。

# 恢复训练设置
save_state = 0                 # 是否保存训练状态，如果设置为1，将在训练期间保存训练状态，例如模型的epoch数。
resume = ""                    # 从指定状态文件夹中恢复训练。需要与上述参数同时使用。因规范文件限制，epoch数和全局步数不会保存，即使在恢复时也会从1开始。

# 其他设置
min_bucket_reso = 256           # 任意最小分辨率，用于某些处理中的最小分辨率限制。
max_bucket_reso = 1024          # 任意最大分辨率，用于某些处理中的最大分辨率限制。
persistent_data_loader_workers = 0  # 持久化数据加载器的工作线程数，可以减少每个epoch之间的停顿，但需要注意可能导致内存占用较大。
clip_skip = 2                   # clip skip 参数，用于梯度裁剪。

# 启动加速器，进行模型训练
accelerate launch ${launchArgs[@]} --num_cpu_threads_per_process=8 "./sd-scripts/train_network.py" \
# 是否使用非正方形图片进行训练 默认为true
  --enable_bucket \
  --pretrained_model_name_or_path=$pretrained_model \
  --train_data_dir=$train_data_dir \
  --output_dir="./output" \
  --logging_dir="./logs" \
  --log_prefix=$output_name \
  --resolution=$resolution \
  --network_module=$network_module \
  --max_train_epochs=$max_train_epoches \
  --learning_rate=$lr \
  --unet_lr=$unet_lr \
  --text_encoder_lr=$text_encoder_lr \
  --lr_scheduler=$lr_scheduler \
  --lr_warmup_steps=$lr_warmup_steps \
  --lr_scheduler_num_cycles=$lr_restart_cycles \
  --network_dim=$network_dim \
  --network_alpha=$network_alpha \
  --output_name=$output_name \
  # 每次送入网络中图片数
  --train_batch_size=$batch_size \
  --save_every_n_epochs=$save_every_n_epochs \
  --mixed_precision="fp16" \
  --save_precision="fp16" \
  --seed="1337" \
  # 是否使用潜在空间的扩散模型进行训练
  --cache_latents \
  --prior_loss_weight=1 \
  --max_token_length=225 \
  # 图片描述信息扩文件
  --caption_extension=".txt" \
  --save_model_as=$save_model_as \
  --min_bucket_reso=$min_bucket_reso \
  --max_bucket_reso=$max_bucket_reso \
  --keep_tokens=$keep_tokens \
  --xformers --shuffle_caption ${extArgs[@]}

```

#### LoRA训练过程
![](https://zhaosi-1253759587.cos.ap-nanjing.myqcloud.com/files/obsidian/picture/202401011220003.png)

#### 参考资料
[什么是LoRA模型，如何使用和训练LoRA模型？你想要的都在这！ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/624230991?utm_id=0)

##  预训练模型网站
- [Hugging Face](https://huggingface.co/models)
- [Civitai](https://civitai.com/)
- [哩布哩布 AI](https://www.liblibai.com/)
## Stable Diffusion 预测

